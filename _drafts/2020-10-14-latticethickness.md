---
layout: post
title: "Lattices which are Good for Covering"
---

(Euclidean) lattices are tightly connected to the notion of *sphere packings*.
"Dual" to this are the notion of *sphere coverings*.
In this post I will discuss a particular construction of lattices which are both
*general*, *explicit*, and has *asymptotically good* (but not optimal)
parameters.
This construction is due to [Davenport in the
50's](https://link.springer.com/article/10.1007/BF02843724), but this paper is
written in the language of *quadratic forms*.
This is entirely equivalent to that of lattices, but one has to "translate"
various various statements.
In this post, in hopes of understanding Davenport's construction better, I will
present it in the language of Euclidean lattices.
My (final) goal is to find some decoding algorithm for this construction.

# Lattice Basics

To try to keep this post concise, I will keep this "introduction to lattices"
rather concise.
Admittedly, it is mostly to fix my notation throughout the rest of this post.
It might be useful to eventually write an introduction to lattices, for now I
will just link to a variety of lecture notes:

* [Daniele Micciancio's course](http://cseweb.ucsd.edu/classes/fa17/cse206A-a/)
* [Oded Regev's
  course](https://cims.nyu.edu/~regev/teaching/lattices_fall_2009/index.html)

It appears that Jonathan Katz and Vadim Lyubashevsky have an [upcoming
book](https://www.amazon.com/Lattice-based-Cryptography-Chapman-Network-Security/dp/1498763472) on the topic, which maybe will be a good pointer in the future.

By a *lattice*, I mean a discrete abelian subgroup of $$\mathbb{R}^n$$, along
with a notion of "distance" (which I will always fix to be the Euclidean norm).
For $$\mathbf{B}\in\mathbb{R}^{k\times n}$$, I write $$\mathcal{L}(\mathbf{B})$$
to be the lattice generated by the *columns* of $$\mathbf{B}$$, i.e.
$$\mathcal{L}(\mathbf{B}) = \mathbf{B}\mathbb{Z}^n$$.
A standard parameter of a lattice $$L$$ is the *shortest vector*, denoted
$$\lambda_1(L) = \min_{\ell\in L\setminus\{0\}} \lVert \ell\rVert_2$$, which is
analogous to the minimum distance of a linear code, and is of fundemental
importance when discussing sphere packing.
As we are interested in the covering problem, I will instead focus on the
*covering radius*.
To define it, I will define the *Voronoi cell* of a lattice $$L$$:

$$\mathcal{V}_L = \{x\in\mathbb{R}^n \mid \forall \ell\in L\setminus \{0\}, \lVert x\rVert_2 < \lVert x - \ell\rVert_2\}$$

This is the set of points which are closer to 0 than any other lattice point.
A basic fact about lattices is that $$\mathbb{R}^n$$ is partitioned by
translates (by lattice vectors) of the Voronoi cell.
Many questions about a lattice can easily be answered by close understanding of
its Voronoi cell, which is a shame, as it is rather hard to compute.

One *particular* question which one can answer is "which points in space are *as
far* from the lattice as possible?".
These are typically called the *deep holes* of the lattice, and their norm is
the *covering radius*.
In particular:

$$\rho(L) = \max_{v\in \mathcal{V}_L} \lVert v\rVert_2$$

The covering radius has a flaw in using it to measure "how good at covering" a
lattice is --- $$\rho(cL) = |c|\rho(L)$$ for any constant $$c$$, so one can just
take $$c$$ arbitrarily large to get lattices that are arbitrarily good at
covering!
For this reason, people often define the *thickness* of a lattice as follows:

$$\Theta(L) = \mathsf{vol}(\mathcal{B}_n(\rho(L))) /
\mathsf{vol}(\mathcal{V}_L)$$

Where $$\mathcal{B}_n(r)$$ is the $$n$$-dimensional Euclidean ball of radius
$$r$$.
This quantity can be seen to be scale-invariant.

There is another somewhat-related lattice parameter which we will care about,
called the *mean-square error* or *second moment*:

$$G(L) = \frac{\int_{\mathcal{V}_L}\lVert x\rVert_2^2
\mathsf{d}x}{\mathsf{vol}(\mathcal{V}_L)}$$

Intuitively, both $$\Theta(L)$$ and $$G(L)$$ measure "how spherical"
$$\mathcal{V}_L$$ is, but in slightly different ways.
With enough work, one can formally connect them (I may give the appropriate
citations later).
Generically, $$\Theta(L)$$ (and other covering-radius related quantities) are
useful for bounding *in the worst case* error which is supported on
$$\mathcal{V}_L$$, and $$G(L)$$ is useful for bounding *in the average case*
error which is *uniformly* distributed on $$\mathcal{V}_L$$ (as it is just the
variance of the uniform distribution on $$\mathcal{V}_L$$).

# Simple Constructions of Lattices

Before discussing Davenport's construction, I will discuss a number of "simple"
constructions of lattices for covering-related problems.
I will casually make references to root lattices such as $$A_n^*, E_8,
\Lambda_{24}$$ --- refer to Conway and Sloane's *Sphere Packings, Lattices, and
Groups*.
Much of the material in this *entire* post can actually be found in that book
(if you look in the right places).

There are two basic operations on lattices that will be important throughout
this post, the sum and the (tensor) product.

## The Sum of Lattices

Given two lattices $$L_0, L_1\subseteq\mathbb{R}^n$$, a natural thing to do is
to add their vectors together (pair-wise).
We write the *Minkowski sum* or *sumset* of two lattices as:

$$L_0 + L_1 = \{\ell_0 + \ell_1\mid \ell_0\in L_0, \ell_1\in L_1\}$$

There are a few cases of particular interest that we briefly mention:

1. If $$\langle L_0, L_1\rangle = \{\langle \ell_0, \ell_1\rangle \mid \ell_0\in
   L_0, \ell_1\in L_1\} = \{0\}$$, we say that the sum is *orthogonal*, and
   notate it $$L_0\perp L_1$$

2. If furthermore the sets $$E_j = \{\vec{e}_i \in
   \mathsf{span}_{\mathbb{R}}(L_j)\}$$ are disjoint, we say that the sum is
   *direct*, and notate it $$L_0\oplus L_1$$.

This later definition is admittedly not great, so I may as well define direct
sums directly (in a way that is hopefully equivalent to the above, but I am not
interested enough in this detail currently to check directly).
For $$L_0\subseteq \mathbb{R}^{n_0}, L_1\subseteq \mathbb{R}^{n_1}$$, we say
their *direct sum* is the lattice $$L_0\oplus L_1 = \{(\ell_0,
\ell_1)\in\mathbb{R}^{n_0 + n_1} \mid \ell_0\in L_0, \ell_1\in L_1\}$$. 

Note that the direct sum itself is orthogonal, so I will focus on this less
restrictive condition briefly.
The orthogonal (and therefore direct) sum is distinguished because
$$\mathcal{V}_{L_0\perp L_1}$$ admits a simple description in terms of
$$\mathcal{V}_{L_0}$$ and $$\mathcal{V}_{L_1}$$.
In particular, it is easy to show that:

$$
\mathcal{V}_{L_0\perp L_1} \supe \{x\in\mathbb{R}^n \mid P_i(L_i)(x)\in P_i(\mathcal{V}_{L_i})\text{ for }i\in\{0,1\}\}
$$

Where $$P_i$$ is the projection onto $$\mathsf{span}_{\mathbb{R}}(L_i)$$.
I actually expect that the above is an equality, but haven't put much effort
into trying to prove it.

As the orthogonal and direct sums are more "geometric" (in that they behave
predictably with respect to Voronoi cells), it is somewhat easier to visualize
them.
The prototypical example of a direct sum is $$\mathbb{Z}^2 = \mathbb{Z}\oplus
\mathbb{Z}$$, and orthogonal sums should vaguely [^orthog] be thought of as
rotated copies of direct sums.

[^orthog]: As $$\mathsf{span}_\mathbb{R}(L)$$ is a vector space, it is
    isomorphic to $$\mathbb{R}^k$$ for some $$k$$. This isomorphism is exhibited
    by some invertible linear map $$T$$, which one can apply the [polar
    decomposition](https://en.wikipedia.org/wiki/Polar_decomposition) to to
    write $$T = UP$$ for $$U$$ unitary and $$P$$ positive semi-definite
    hermitian.
    Finally, if we hit our lattices with $$U$$, I think it should "align" them
    such that $$U(L_0\perp L_1) = (UL_0)\oplus (UL_1)$$, but am not fully sure.

## The Product of Lattices

For lattices $$L_0, L_1 = \mathcal{L}(\mathbf{B}_0),
\mathcal{L}(\mathbf{B}_1)$$, 

Key to much of this post will be the lattice:

$$k\mathbb{Z}^n + \mathcal{L}(\vec{1})$$

Equivalently, this is the lattice $$\mathcal{L}([kI_n, \vec{1}^t])$$.
It is not hard to show that (under elementary column operations) this lattice
has the basis:

$$\mathbf{B} = \begin{pmatrix}k & 0 & 0 & \dots & 1\\ 0 & k & 0 & \dots & 1 \\ 0
& 0 & k & \dots & 1\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 &
\dots & 1\end{pmatrix}$$

I.e. it is $$k$$ times the identity matrix, except for the last column which is
the all 1's vector.
If $$k = 2$$, this is precisely the lattice $$2D_n^*$$.
For $$k \neq 2$$ the story is slightly more exciting [^divis], but one can write (as long
as $$k \mid n$$ --- if this does not occur, one can still describe the lattice,
but it is more complex):

[^divis]: We additionally need the condition $$k\mid n$$. If this
    condition does not hold, one can still explicitly describe the lattice, but
    you get that it is $$\bigcup_{g\in G}g + kA_{n/m} \perp
    \mathcal{L}(\vec 1)$$, where $$A_{n/m}$$ is a "Coxeter lattice", and $$G$$
    is a group of size $$n / \mathsf{gcd}(k, n)$$ that I do not want to
    describe.

$$k\mathbb{Z}^n + \mathcal{L}(\vec{1}) = kA_{n-1}^* \perp \mathcal{L}(\vec 1)$$

Where $$\perp$$ states that $$\langle kA_{n-1}^*, \mathcal{L}(\vec 1)\rangle =
0$$.
One could further rotate this lattice to make $$\mathcal{L}(\vec 1)$$ align with
a coordinate axis so that this is a direct sum, but I will not be interested in
doing this.

**NOTE**: I think the tensor product should play well with the direct and
orthogonal sums.
Focusing on the case of orthogonal sums, one can see via matrix manipulations
that $$A\otimes\bigoplus_{i = 1}^n L_i = \bigoplus_{i = 1}^n A\otimes L_i$$.
This is probably enough to show it for orthogonal sums as well, provided my idea
that "orthogonal = direct + rotation" is true.
If this is the case, then Davenport's construction $$D_{n, p}\otimes L = (A_{n-1}^*\perp \frac{1}{p}\mathcal{L}(\vec{1}))\otimes L = A_{n-1}^*\otimes L \perp \frac{1}{p}\mathcal{L}(\vec{1})\otimes L$$.
Decoding the lattices in these summands seems close to being feasible, as
$$\mathcal{L}(\vec{1})\otimes L$$ should be roughly equal to $$e_1\otimes
\mathcal{L}\cong\mathcal{L}$$, so the problem reduces to decoding
$$A_{n-1}^*\otimes L$$ for a fixed "base" lattice $$L$$, which is likely $$A_8,
E_8, \Lambda_{24}$$.
If one uses $$A_8^*$$
